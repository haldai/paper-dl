#!/usr/bin/env bash
source paper-dl-util

parse_args "$@" && check_args && initial

case $PROCEEDING in # extract the name and url of the proceeding
'nips')
    SAVE_DIR=$SAVE/$(cat $CACHE/index-$server_ver.html | sed -n "s/.*\".\(.*NIPS $YEAR.*\).\/a.*/\1/p")
    PROCEEDING_URL=$(cat $CACHE/index-$server_ver.html | sed -n "s/.*href=\"\(.*$YEAR\)\".*/\1/p")
    ;;
'pmlr')
    SAVE_DIR=$SAVE/$(cat $CACHE/index-$server_ver.html | sed -n "s/.*Volume $VOLUME.\/b..\/a.\(.*\)/\1/p" | sed -e 's/^[ ]*//g' | sed -e 's/[ ]*$//g')
    PROCEEDING_URL=$(cat $CACHE/index-$server_ver.html | sed -n "s/.*href=\"\(.*\)\".*Volume $VOLUME.\/b..\/a.*/\1/p")
    ;;
'jmlr')
    SAVE_DIR=$SAVE/$(cat $CACHE/index-$server_ver.html | sed -n "s/.*volume\".\([Volume ]*$VOLUME\).\/font.*/\1/p" | sed -e 's/^[ ]*//g' | sed -e 's/[ ]*$//g')
    PROCEEDING_URL=$(cat $CACHE/index-$server_ver.html | sed -n "s/.*href=\"\(.*\)\"..font class=\"volume\".[Volume ]*$VOLUME.\/font.*/\1/p" | sed -e 's/\/papers\///g')
    ;;
esac

if [ -z "$PROCEEDING_URL" ]; then
    echo -e "${RED}Warning: no proceedings meet the query requirements!${NC}" && exit 1
fi

mkdir -p "$SAVE_DIR"

if [ ! -f "$CACHE/$YEAR.html" ]; then
    echo -e "${YELLOW}::${NC} Cache the proceeding page"
    wget -t 0 -q --show-progress -O "$CACHE/$YEAR.html" $HOST$PROCEEDING_URL
fi

# extract the content of $YEAR.html and save to $YEAR-$VERSION.txt
# $YEAR-$VERSION.txt format: title@author@url, where @ is the separator
# url format: Paper~paper_url[,Supplemental~supplemental_url,Bib~bib_url,Erratum~erratum_url]
if [ ! -f "$CACHE/$YEAR-$VERSION.txt" ]; then
    case $PROCEEDING in
    'nips')
        cat "$CACHE/$YEAR.html" | grep '/paper/' | awk -F '</a> ' -v y="$YEAR" '{
            gsub("%", "%%", $1); gsub("</?var>", "$", $1); gsub("[ ]+", " ", $1);
            split($1, arr, ">"); printf arr[3]"@";
            gsub("<a href=[^>]*>", "", $2); gsub("</a>[</li>]*", "", $2); printf $2"@";
            gsub("/paper/", "http://papers.nips.cc/paper/", $1); split($1, arr, "\""); printf "Paper~"arr[2]".pdf,Bib~"arr[2]"/bibtex";
            if (y > 2013) printf ",Supplemental~"arr[2]"-supplemental.zip\n"; else printf "\n";
        }' >"$CACHE/$YEAR-$VERSION.txt"
        ;;
    'pmlr')
        cat "$CACHE/$YEAR.html" | hxnormalize -l 1000 -x | grep -E '<p class="title">|<span class="authors">|<p class="links">' | awk -F '</a> ' '{
            if (index($0, "<p class=\"title\">") != 0) { split($0, arr, ">"); split(arr[2], arr2, "<"); printf arr2[1]"@"; }
            if (index($0, "<span class=\"authors\">") != 0) { split($0, arr, "> "); split(arr[3], arr2, " <"); printf arr2[1]"@"; }
            if (index($0, "<p class=\"links\">") != 0) {
                split($0, arr, "\""); 
                for (i in arr) {
                    if (index(arr[i], "-supp.pdf") != 0) printf ",Supplemental~"arr[i];
                    else if (index(arr[i], ".pdf") != 0 && index(arr[i], "PDF Downloads") == 0) printf "Paper~"arr[i];
                }
                printf "\n";
            }
        }' >"$CACHE/$YEAR-$VERSION.txt"
        ;;
    'jmlr')
        cat "$CACHE/$YEAR.html" | sed -e :a -re 's/<!--.*?-->//g;/<!--/N;//ba' | recode html..utf8 | hxnormalize -l 1000 -x | grep -E '<dt>|<b>[ ]*<i>|>abs<|.pdf' | awk -F '\"' '{
            gsub("href=\"/papers", "href=\"http://www.jmlr.org/papers");
            gsub("<b>[ ]*<i>", "<b><i>");
            if (index($0, "<dt>") != 0) {
                if (NR != 1) { printf author"@"url"\n"; }
                split($0, arr, ">"); split(arr[2], arr2, "<");
                a = arr2[1]; gsub("    ", "", a); printf a"@";
                author=""; url="";
            }
            if (index($0, "<b><i>") != 0) { split($0, arr, "<b><i>"); split(arr[2], arr2, "</i></b>"); author=author""arr2[1]; }
            for (i = 2; i < NF; i++) {
                if (index($i, ".zip") != 0) url = url",Supplemental~"$i;
                else if (index($i, "appendix") != 0 && index($i, ".pdf") != 0) url = url",Supplemental~"$i;
                else if (index($i, "errat") != 0 && index($i, ".pdf") != 0) url = url",Erratum~"$i;
                else if (index($i, "rev1") != 0 && index($i, ".pdf") != 0) url = url",Revision~"$i;
                else if (index($i, "_full.pdf") != 0) url = url",Full~"$i;
                else if (index($i, ".pdf") != 0) url = url",Paper~"$i;
                else if (index($i, ".bib") != 0) url = url",Bib~"$i;
            } 
        } END { printf author"@"url"\n"; }' >"$CACHE/$YEAR-$VERSION.txt"
        ;;
    esac
fi

# generate query command
command="cat \"$CACHE/$YEAR-$VERSION.txt\"" # show all the papers
if [ -n "$KEYWORD" ]; then
    for KW in ${KEYWORD[@]}; do
        command="$command | grep -E '$KW|${KW,}|${KW,,}|${KW^}|${KW^^}'"
    done
elif [ -n "$AUTHOR" ]; then
    author=$(echo $AUTHOR | sed 's/.*/\L&/; s/[a-z]*/\u&/g') # title case
    command="$command | grep '$author'"
fi

# show query results in descending order
LOCAL_PAPER=$(ls "$SAVE_DIR" | sed -e 's/\\//g') # local downloaded paper list, delete all \

eval $command | awk -F '@' -v LP="$LOCAL_PAPER" '
    BEGIN { i = 0; } { title[i] = $1; author[i] = $2; url[i] = $3; i++; } 
    END { for (i = NR-1; i >= 0; i--) {
            t = title[i]; gsub("\\\\", "", t); gsub("/", "#", t);
            if (index(LP, t) != 0) printf "\033[0;35m%d\033[0;37m %s \033[0;33m(downloaded)\033[0;0m\n", i+1, title[i];
            else printf "\033[0;35m%d\033[0;0m %s\n", i+1, title[i];
            printf "    \033[0;36m\033[3m%s\033[23m\033[0;0m\n   ", author[i];
            split(url[i], arr, ","); 
            for (j in arr) {
                if (index(arr[j], "~") != 0) { split(arr[j], arr2, "~"); printf " [\033[1;34m%s\033[0;0m]", arr2[1]; }
            }
            printf "\n";
        }}'

select=($(eval $command | awk -F '@' -v LP="$LOCAL_PAPER" '{ 
    gsub("\\\\", "", $1); gsub("/", "#", $1); 
    if (index(LP, $1) != 0) print 0; else print 1; 
    }'))

query_total=${#select[@]}
select_total=$(sum ${select[@]})
downloaded=$(($query_total - $select_total))

if [ $query_total = 0 ]; then
    echo -e "${RED}Warning: no papers meet the query requirements!${NC}"
elif [ $downloaded = $query_total ]; then
    echo -e "${RED}==> All papers have been downloaded!${NC}"
else
    if [ $downloaded = 0 ]; then
        echo -e "${YELLOW}==> Papers to download (eg: 1 2 3, 1-3 or ^3), default all ($query_total)${NC}"
    else
        echo -e "${YELLOW}==> Papers to download (eg: 1 2 3, 1-3 or ^3), default all ($query_total-$downloaded=$select_total)${NC}"
    fi
    read -p $'\033[33m==> \033[0m' select_options
    if [ -n "$select_options" ]; then
        select=($(parse_select_options "$select_options" $query_total))
        select_total=$(sum ${select[@]})
    fi

    # start download
    if [ "$select_total" -gt 0 ]; then
        cd "$SAVE_DIR" && i=0 && j=0
        for line in $(eval $command | tr ' ' '%>_<%'); do
            if [ "${select[$i]}" = 1 ]; then

                line=$(echo "$line" | tr '%>_<%' ' ')
                title=$(echo "$line" | cut -d '@' -f 1)
                author=$(echo $line | cut -d '@' -f 2)
                url=$(echo $line | cut -d '@' -f 3)

                ((j++))
                awk 'BEGIN{printf "\033[35m\n%d/%d (%.2f%)\033[0m", '$j', '$select_total', (100 * '$j'/'$select_total')}'
                echo -n " ${title} "
                echo -e "${CYAN}\033[3m$author\033[23m${NC}"

                title=${title//\//#} && mkdir -p "$title" && cd "$title"

                url=($(echo $url | tr ',' ' '))
                for var in ${url[@]}; do
                    file_type=$(echo $var | cut -d '~' -f 1)
                    file_url=$(echo $var | cut -d '~' -f 2)

                    echo -e "${YELLOW}::${NC} Download $file_type"
                    wget -t 0 -q --show-progress -P . $file_url

                    if [ $file_type == 'Paper' ]; then # get paper name for the following merge step
                        paper_name=$(echo $file_url | sed -e 's/\(.*\/\)//g')
                    fi

                    if [ $file_type == 'Supplemental' ] || [ $file_type == 'Erratum' ]; then
                        supp_name=$(echo $file_url | sed -e 's/\(.*\/\)//g')
                        ext_name=$(echo $supp_name | sed -e 's/\(.*\.\)//g')
                        if [ -f "$supp_name" ]; then # if supp file exists
                            case ${ext_name,,} in
                            'pdf')
                                gs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile="${paper_name%.pdf}-merge.pdf" "$paper_name" "$supp_name"
                                ;;
                            'zip')
                                merge_command="gs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile='${paper_name%.pdf}-merge.pdf' '$paper_name'"
                                mkdir -p supp && unzip -q "$supp_name" -d supp && flag=0
                                for line in $(find supp -maxdepth 3 -regextype posix-extended -iregex '.*\.(pdf)' | grep -v "MACOSX" | tr ' ' '@@'); do
                                    if [ -n "$line" ]; then
                                        merge_command="$merge_command '$line'"
                                        flag=1
                                    fi
                                done
                                if [ "$flag" = 1 ]; then
                                    merge_command=$(echo "$merge_command" | tr '@@' ' ')
                                    eval "$merge_command"
                                fi
                                rm -rf supp
                                ;;
                            esac
                        fi
                    fi
                done
                cd ..
            fi
            ((i++))
        done
    else
        echo -e "${RED}Warning: there is nothing to do!${NC}"
    fi
fi
